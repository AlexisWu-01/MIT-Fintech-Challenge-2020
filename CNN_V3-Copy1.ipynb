{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer as MLB\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxoutDense\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import preprocessing\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_small():\n",
    "    X_train = pd.read_csv(\"data_small/X_train_small.csv\",skiprows = 1,header = None)\n",
    "    X_test = pd.read_csv(\"data_small/X_test_small.csv\",skiprows = 1, header = None)\n",
    "    y_train = pd.read_csv(\"data_small/y_train_small.csv\", header=None)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train\n",
    "#     return np.dstack(X_train), np.dstack(X_test), np.dstack(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_big():\n",
    "    X_train = pd.read_csv(\"data_big/X_train_big.csv\",skiprows = 1,header = None)\n",
    "    X_test = pd.read_csv(\"data_big/X_test_big.csv\",skiprows = 1, header = None)\n",
    "    y_train = pd.read_csv(\"data_big/y_train_big.csv\", header=None)\n",
    "    return X_train,y_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train = read_data_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train,X_test,y_train = read_data_small()\n",
    "    \n",
    "\n",
    "        \n",
    "    for i in range(2):\n",
    "        X_train[i+1] = pd.get_dummies(X_train[i+1])\n",
    "        X_test[i+1]= pd.get_dummies(X_test[i+1])\n",
    "\n",
    "    for i in range(9):\n",
    "        X_train[i+8] = pd.get_dummies(X_train[i+8])\n",
    "        X_test[i+8]= pd.get_dummies(X_test[i+8])\n",
    "    X_train.drop([3,4,5],1,inplace = True)\n",
    "    X_test.drop([3,4,5],1,inplace = True)\n",
    "#     for i in range(X_train.shape[1]):\n",
    "#         X_train[i].fillna(X_train[i].mean())\n",
    "    \n",
    "    X_train.fillna(X_train.mean(axis = 1,skipna = True),inplace = True)\n",
    "    y_train.fillna(y_train.mean(axis = 1,skipna = True),inplace = True)\n",
    "    X_test.fillna(X_test.mean(axis = 1,skipna = True),inplace = True)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "    X_test = X_test.values\n",
    "    X_test_scaled = min_max_scaler.fit_transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_scaled)\n",
    "    X_test = pd.DataFrame(X_test_scaled)\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtn,xte,ytn = data_preprocessing()\n",
    "xtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    X_train, X_test, Y_train = data_preprocessing()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_train,Y_train,test_size = 0.2)\n",
    "      \n",
    "    return x_train.to_numpy(),y_train.to_numpy(),x_test.to_numpy(),y_test.to_numpy()\n",
    "\n",
    "#     return x_train.to_numpy(),y_train.to_numpy(),x_test.to_numpy(),y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_weight(lst):\n",
    "    weight = []\n",
    "    for item in lst:\n",
    "        if item.tolist() == [1.0,0.0,0.0]:\n",
    "            weight.append(1)\n",
    "        elif item.tolist() == [0.0,1.0,0.0]:\n",
    "            weight.append(384)\n",
    "        else:\n",
    "            weight.append(864)\n",
    "    return weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_kappa(y_true, y_pred):\n",
    "    y_true_classes = tf.argmax(y_true, 1)\n",
    "    y_pred_classes = tf.argmax(y_pred, 1)\n",
    "    return tf.contrib.metrics.cohen_kappa(y_true_classes, y_pred_classes, 10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    trainX, trainy, testX, testy = split_data()\n",
    "    trainy = encoder.fit_transform(trainy).toarray()\n",
    "    testy = encoder.fit_transform(testy).toarray()\n",
    "    sample_weight1 = np.asarray(generate_sample_weight(trainy))\n",
    "    sample_weight2 = np.asarray(generate_sample_weight(testy))\n",
    "    trainX = np.reshape(trainX,(trainX.shape[0],1,trainX.shape[1]))\n",
    "    testX = np.reshape(testX,(testX.shape[0],1,testX.shape[1]))\n",
    "    verbose,epochs,batch_size = 1, 10, 16\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features),padding = 'same',kernel_initializer = 'he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3, padding = 'same'))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#     model.add(Dense(100,activation='relu'))\n",
    "#     model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    \n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Dense(500,activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,sample_weight = sample_weight1,validation_data = (testX,testy))\n",
    "    model.save('CNN_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit49a3228577b048f9be15ae6d75dea926"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
